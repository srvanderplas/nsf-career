---
title: "Project Description"
bibliography: refs.bib
editor: 
  markdown: 
    wrap: sentence
---

::: {.content-visible when-format="pdf"}
\newsection{D}
:::

```{r new, echo=F, fig.width=2, fig.height=1.5,warning=F,message=F,out.width=".25\\textwidth", include = F}
nums<-rnorm(100,0,1)
df<-data.frame(s=1:100,nums)
library(ggplot2)
ggplot(df, aes(x=nums))+
  geom_histogram()+
  theme_classic(base_size=9)
```

<!-- https://vcresearch.berkeley.edu/sites/default/files/inline-files/CAREER_Writing_Guide_2017_final.pdf -->

# Overview

<!-- 1. Objectives/Specific Aims/Goals -->

<!-- Other names for these are goals, research thrusts, etc. You can call these whatever you like or whatever is most used in your field. -->

<!-- • Try to fit this information on one page. -->
<!-- • Start with a brief problem statement to introduce your research question and state why it is important. -->
<!-- • Put the solution to your research question in context of your overall, long-term career objectives. -->
<!-- • Identify the specific objective for the present proposal. -->
<!-- • Describe your overall hypothesis for your specific objective. -->
<!-- • List your specific aims for how you will accomplish the objectives of your proposal.  -->
<!--    o Each aim is to find out information, not to do a method (a method supports an aim, but is not the purpose of the aim; i.e., the aim should be outcome-oriented). -->
<!--    o Limit yourself to 2-4 aims. -->
<!--    o Be declarative (use short bullet points). -->
<!--    o Make sure aims are not inter-dependent but supportive of each other (i.e., make sure that if the first step of your proposal doesn’t turn out the way you expect, your entire proposal won't fail). -->
<!--    o Link your specific aims to hypotheses, as appropriate. -->
<!-- • Close this section with a statement on your expected outcomes, emphasizing the project's innovation. -->
<!-- • It can be very useful to try to diagram your objective and aims. -->
 

<!-- Overview and Objectives (Suggested length: 1-1.5 pages)  -->

 
<!-- Problem statement:  -->
<!-- Provide a succinct statement of the problem or opportunity your proposal will address.   -->
Statistical graphics and models are powerful tools to summarize data and support human decision making; however, empirical research on graphical perception is sparse relative to the number of decisions necessary to make a good chart. When relevant studies are available, they often use incomparable methods and produce conflicting results.
Chart design guidelines are often based on opinion, not empirical study, rendering many scientific communications sub-optimal or ineffective.
This is alarming: effective science communication is critical for cultivating public trust in the scientific process and ensuring that decision makers accurately interpret supporting information. 
<!-- State a long-term goal that includes both research and education.   -->
Addressing these challenges, my long-term career goal is to examine statistical graphics with the goal of *helping people use data more effectively*, and to apply this research to educate and inspire a new generation of scientists while supporting science literacy among the general public. 

<!-- Briefly address how your proposed research and education activities will help synthesize, build, and/or expand foundations in the relevant areas.  -->
This CAREER proposal addresses a fundamental research question underpinning this problem: *How do design decisions impact the use, design, and perception of data visualizations?* Three research objectives support this goal:

<!-- State the research objectives of the proposed work, along with any relevant hypotheses and rationales.  -->

-   **RO1:** Create a framework for comprehensive graphical testing across multiple levels of user engagement.
-   **RO2:** Assess the impact of measurement methods on experiments evaluating statistical graphics.
-   **RO3:** Empirically validate common chart design guidelines, measuring the impact of design decisions on task performance.

Integrated with these research efforts, the overall **education goal** is to leverage visualization research to motivate statistical learning and improve data-driven decision making in society. Three education objectives (EOs) address this goal:

<!-- State the education objectives of the proposed work, along with any relevant hypotheses and rationales.  -->

-   **EO1:** Develop and implement experiential learning activities in graphics for undergraduate introductory statistics courses.
-   **EO2:** Create graduate course modules for K-12 educators that connect ongoing research to engaging, hands-on classroom activities for teaching statistics, math, and science.
-   **EO3:** Improve the penetration of visualization research beyond academia by incorporating summaries of empirical studies in resources used by data scientists, industry analysts, and researchers in STEM disciplines.

<!-- Describe how the research and educational activities are integrated with one another or synergistic.   -->
Experiential learning activities will connect graphics research to critical concepts within statistics courses at the undergraduate level as well as in K-12 activities provided during graduate coursework for STEM educators. 
In addition, incorporating research summaries into general visualization resources will not only connect data visualization creators with research; improving these resources will improve teaching materials for statistical computing and will involve undergraduates in research and outreach in graphics and science communication. 

The research and educational activities described in this project have the potential to significantly improve how scientists communicate scientific results to each other as well as to the general public, increasing public trust in science and facilitating public decision making based on experimental data and results. 

# Intellectual Merit
<!-- Intellectual Merit (Suggested length: 1/4-1/2 page)  -->
<!-- Describe the potential of the project to advance knowledge within its own field or across different fields.  -->
This work will expand our understanding of graphical perception and communication by empirically and systematically examining chart design through comprehensive, task-based testing.
<!-- Describe the expected significance of your project with respect to the research plan.   -->
The proposed studies will be used to
generate a framework relating evaluation methods to user engagement with graphics,
establish the impact of different experimental design decisions on results, and 
promote integration of multiple evaluation methods
<!-- incorporating elements of quantitative and qualitative feedback  -->
to provide a holistic assessment of visualization effectiveness.
Additionally, this project will prioritize inclusion neurodiverse and disabled individuals, ensuring that design guidelines account for accessibility concerns.
<!-- beyond creation of alt-text for visual impairments or colorblind-friendly palette selection. -->
<!-- Explain the extent to which the proposed activities suggest and explore creative, original, or potentially transformative concepts.  -->
The results of the systematic examination of different experimental design and testing methods will not only ground design guidelines in empirical results; if successful, the experiments will also help reconcile the results from historical studies with conflicting results.
While there are task-based taxonomies for *selection of chart types*, a systematic framework for selecting *testing methods* based on levels of engagement and critical tasks is innovative; we expect that this framework will facilitate well-rounded experiments that examine chart design and use from multiple perspectives, providing nuanced results focused on audience use of graphics.
<!-- When possible, describe how your education plan will contribute to new knowledge (e.g., expected contributions to the field of STEM education).  -->
The education activities proposed in this project are closely tied to the research objectives, providing avenues for dissemination of research results as well as inclusion of audiences in graphics research. 
As a result, education and research activities will combine to support new pedagogical research in experiential learning. 
This new research will examine the use of statistical graphics as an entry-point to quantitative subjects for individuals who are not traditionally interested in pursuing STEM careers. 
<!-- Discuss your qualifications to lead the project.  -->
Previous collaborative research projects have established new and re-imagined old methods for testing statistical graphics; when combined with training and experience in statistics at the intersection of computer science, psychology, and communication, I am well equipped to complete this project supported by collaborations with researchers in cognitive psychology and statistical education. 


<!-- Relation to Principal Investigator’s Long-term Goals (Suggested length: 1/4-1/2 page)  -->
<!-- Restate the long-term goal of your research program and describe how the proposed work advances that long-term goal.  -->
My long-term career goal is to examine statistical graphics with the goal of *helping people use data more effectively*, and to apply this research to educate and inspire a new generation of scientists while supporting science literacy among the general public. 
<!-- Explain how the proposed project will build a firm foundation for a lifetime of leadership in research, education, and their integration.  -->

<!-- Explain how the proposed project will advance your career goals and job responsibilities as well as the mission of your department or organization.  -->

# Research Plan
<!-- Note: The research and educational activities do not need to be addressed separately if the relationship between them is such that it works best to present an integrated project where the two are interspersed throughout the Project Description. -->

## Background
<!-- (Suggested length: 2-4 pages)  -->
<!-- This section lets you: -->
<!-- 1. Orient reader to your subject, providing scientific background and context. -->
<!-- 2. Establish the importance and novelty of your project. -->
<!-- 3. Show your knowledge of the area through a solid review and objective citation of prior related work. -->
<!-- 4. Reveal that you are aware of opportunities, gaps, and roadblocks in your field.  -->
<!-- 5. Persuade the reviewers to become invested in your work by showing how your research matters for the field and their own research. -->
<!-- Write this section in nontechnical terms for a broader audience. -->

## Overview 

<!-- Briefly sketch the background leading to the present application, critically evaluate the existing literature, and identify gaps the project is intended to fill.   -->
Scientific graphics transform quantitative data into image representations that can make use of the human visual system, leveraging our ability to take in and process huge quantities of information with minimal cognitive effort.
However, unlike many mathematical data transformations, the transformation to visual space incurs loss both in the rendering of data to image and the transition from image to cognitive representation. 
That is, when creating data visualizations, we have to be concerned not only with the accuracy of the rendered image, but also with how that image is perceived by the viewer.
It is easy to find entire books filled with situations in which the transition from data to image produces results which are misleading
[@cairoTruthfulArtData2016; @cairoHowChartsLie2019; @huffHowLieStatistics1954]; identifying scenarios where the transition from image to cognitive representation is suboptimal is more challenging and requires user studies. 
There have been empirical studies of graphics for at least 100 years [@croxtonBarChartsCircle1927; @eellsRelativeMeritsCircles1926; @wickham2013graphical], but the foundational work in graphical perception is @cleveland1984, which established viewer's ability to accurately estimate information from simple visual displays. 
While this work is important, and valuable, it has been synthesized into recommendations and rankings which go far beyond the original experiments [@mackinlayAutomatingDesignGraphical1986; @franconeriScienceVisualData2021] with limited empirical verification, though in many cases these extrapolations are based in part on cognitive and perceptual research that is not specific to scientific visualization. 
It is easy to forget that @cleveland1984 examined charts with respect to the direct numerical accuracy of quantitative estimates; the results do not necessarily apply if we are interested instead in determining whether differences between quantities can be perceived [@luModelingJustNoticeable2022;@rensinkPerceptionCorrelationScatterplots2010], ordered, remembered [@borkinMemorabilityVisualizationRecognition2016], or used to reach a reasonable real-world decision [@kellerEffectRiskCommunication2009]. 
The design space of visualization user studies is incredibly large , and studies may use different numerical measures to address the same basic question.
While each of these alternate tasks has been addressed in user studies of graphics, because the design space of visualization user studies is so large [@Abdul-Rahman2020;@Bolte2020] and the literature is spread across so many different fields (including psychology, computer science, statistics, design, and communication) with different standard methods, it is extremely difficult to synthesize the total graphics literature in order to derive empirically driven guidelines for creating graphs that accurately transform the data into an image and also present the data in a form which can be effectively used by the intended audience.


![Levels of cognitive engagement with charts, roughly ordered by complexity, time, and effort. Methods which effectively measure (or could be extended to measure) each stage are shown below the charts. Text annotations show examples of the types of operations which involved in each stage.](images/Chart-Perception-Process.png){#fig-cognition-hierarchy}

<!-- Discuss how this project will generate foundational research that will advance the field in general or address specific challenges of great importance.   -->
The research objectives proposed here are designed to lay a foundation for more consistent evaluation and testing of scientific visualizations. 
We focus on the integrated cognitive complexity and temporal evolution of user-chart interaction, which is roughly illustrated in @fig-cognition-hierarchy. 
Previous hierarchies have focused on the complexity of single graphical tasks [@carswellChoosingSpecifiersEvaluation1992; @cleveland1984; @spenceVisualPsychophysicsSimple1990]; while this is a useful way to determine which chart to use to display data, it does not approach different ways users engage with a single chart: 
are they perceiving the graphical forms without engaging with the underlying symbolic meaning? 
Using the chart to understand the underlying natural phenomenon? 
Doing statistical inference (e.g. visually estimating parameter values from the graph)? 
Making decisions based on their understanding of the data? 
Each of these use cases involves different cognitive tasks, and as a result, different graphical testing methods must be used to assess the effectiveness of charts under each type of engagement. 

We will first identify and evaluate methods for graphical testing across multiple levels of user engagement, comparing methods which examine equivalent stages of graph comprehension and use. 
\@fig-cognition-hierarchy shows some of the methods we intend to assess and compare, along with the rough stages of cognition these methods target. 
Next, we will establish the impact of different experimental configurations and ways of measuring and recording users' answers. 
We expect that this will not only help graphics researchers design and implement new user studies, but we hope to also facilitate comparison of results from past studies, providing context to conflicting conclusions.
Finally, we will empirically validate common chart design guidelines, testing whether extrapolated results and aesthetic opinions hold up under critical user studies.

<!-- Describe the contributions the project will make to synthesizing, expanding, or building the base of knowledge and evidence needed in the field and to the development of theory and methodology.  -->
The results from these objectives, taken together, are intended to build a user-focused foundation for measuring and assessing the design and use of data visualizations. 
<!-- Because scientific visualizations are used by and leverage research from so many different disciplines, including cognitive psychology, computer science, communication, human-computer interaction, user interface design, and statistics, it can be difficult to identify the exact theoretical domains to which a statistical graphics research project contributes.  -->
The choice to approach testing graphics from the perspective of how the user is interacts with and makes decisions based on the visual representation of the data places this research firmly at the intersection of statistics, cognitive science, measurement, and communication. 
While previous researchers [@shneidermanEyesHaveIt1996; @carswellChoosingSpecifiersEvaluation1992; @cleveland1984] have assessed graphics from the perspective of different estimation or user interaction tasks, the present project is focused on **measurement** methods for different stages of user interaction with graphics. 
Thus, this project will develop methodology for measuring the functional cognition underlying data driven decision making using visual aids. 
The results from the proposed research will also allow integration of conflicting historical results, hopefully leading to a robust set of empirical evidence that can be integrated to produce more robust, task-focused design guidelines for statistical graphics. 


## Motivation

<!-- Provide a brief review of the pertinent literature to show the current state of the field and to illustrate the gap your proposed work will fill.  -->

The first studies experimentally examining the effectiveness of statistical graphics took place approximately 100 years ago; since then, the quantity of charts created, the methods available for creating charts, and the technology available for measuring and evaluating comprehension have evolved in remarkable ways. @vanderplasTestingStatisticalCharts2020 provides a comprehensive review of studies that experimentally examine the use of statistical graphics as well as the underlying research in cognitive psychology topics such as perception, memory, attention, and executive function which influence our ability to use statistical graphics effectively. 

What is remarkable given the ubiquity of statistical graphics in scientific communication is that even after 100 years of empirical graphics research, we still have relatively little empirical evidence to support of some common design guidelines and heuristics; where there are empirical studies, they often conflict or have been over-extrapolated from the design and goal of the original experiments. 
For example, Tufte's data-ink ratio @tufteVisualDisplayQuantitative2001 has been thoroughly tested [@kellyDataInkRatioAccuracy1988;@spenceVisualPsychophysicsSimple1990;@carswellChoosingSpecifiersEvaluation1992;@gillanMinimalismSyntaxGraphs1994;@gillanMinimalismSyntaxGraphs2009], but results have been decidedly mixed, suggesting that the data-ink ratio is too simplistic; even so, it is still part of the common vernacular and makes its way into many different design guidelines [@ajaniDeclutterFocusEmpirically2022]. 
Another common recommendation is to locate the most important variables along position axes (e.g. $x$ and $y$ in a scatterplot) rather than encoding quantitative information in color; this is because @cleveland1984 found higher levels of accuracy in these comparisons, but accuracy of numerical estimation is not the only important way people use charts @bertiniWhyShouldnAll2020, and in fact, it is relatively uncommon for individuals to directly estimate one specific numerical quantity from a chart: for these tasks, a table would be much more appropriate @gelmanWhyTablesAre2011.

At a fundamental level, we know that graphics are useful for communicating scientific results and for exploring our data; whether the target audience is ourselves, peers, or the general public, graphics are an invaluable tool.
So why do we assess graphics based on things like estimation accuracy or response time [@hullmanBenefittingInfoVisVisual2011], and then extrapolate the results to tasks and situations that don't revolve around estimation accuracy? 
What is needed instead is a testing framework focused on the user's level of interaction and purpose for interacting with a chart. 
@lamEmpiricalStudiesInformation2012 divides evaluation scenarios into several user-focused task-based methods for both visualization and data analysis, assessing the utility of several methods for testing these empirically, but stops short of actually performing experiments evaluating the same graphics using multiple different methods. 
This component of the proposed work is essential because it provides multiple points of experimental control that are not present when aggregating results across experiments: it is possible to keep the same participants, data (or data generating model), and testing conditions across multiple testing methods. 
In this work, we propose a comprehensive, multi-modal experimental framework for evaluating graphics.
This will provide a better alternative to the patchwork testing of individual questions with highly specific methods by empirically assessing how specific charts (or design decisions) function under different tasks and measurement methods.


There are multiple factors that must be considered and evaluated in order to achieve the broader goal of empirically testing design guidelines: the measurement methods and variables used to assess charts are of obvious interest, but other factors are also important. 
Measurement of numerical information that has passed through the human brain in one form or another can be complicated by the method used to obtain and record the information. 
Consider the relatively simple case where a participant is asked to estimate the length of a specified bar in a bar chart: the experimenter must determine how this estimate is recorded. 
Modern web design (assuming our experiment is conducted online or at least that data is entered via  a computer interface) provides multiple options: the user can directly enter a number in a text box or indicate the number on a slider (with or without anchor points); the former requires translation into an explicitly numerical domain, where the latter requires that the participant map the chart onto a spatial domain but does not necessarily require explicit formation of a numerical estimate. 
Direct entry is subject to rounding effects that increase with participant uncertainty [@ruudUncertaintyCausesRounding2014;@hondaNumberBiasWisdom2022]; while these effects can be mitigated @wangDensityEstimationData2013 through modeling, it might be preferable to make use of numerical inputs that might not trigger rounding, such as slider inputs.
Unfortunately, slider inputs are not entirely simple either: they can contain anchor points (or not) that participants may latch on to; the inclusion of these additional annotations may reduce cognitive load, but may provide the opportunity for additional anchoring effects that must be considered and possibly modeled.
Most research in this area has examined sliders as inputs for categorical variables[@thomasSliderScaleText2019;@liuWhereShouldStart2019;@funkeWebExperimentShowing2016a;@decastellarnauClassificationResponseScale2018;@couperEvaluatingEffectivenessVisual2006] and suggests that using sliders instead of radio button inputs changes the observed distribution of responses in important ways; while the comparison to radio buttons is not relevant to continuous data, the results of these studies suggest that there is a need to explicitly examine the effects of input methods on participant responses.
This is just one example of the series of decisions experimenters make about the process of elucidating and recording data from participants which do not directly relate to the hypotheses under investigation but that may well impact the results.
Even numerical estimation (regardless of input method) can be tricky: @spenceVisualPsychophysicsSimple1990 asks participants for the ratio between the part and the whole (e.g. estimating the ratio $A/B$ where $A+B=1$), while @cleveland1984 asked participants to directly estimate the value of $A$; it seems possible that the different conclusions regarding the accuracy of pie charts might in fact be a result of the quantity participants were asked to investigate.


Combining the toolbox of methods for testing graphics at different levels of user engagement and the assessment of measurement details that impact research in statistical graphics but are not directly of interest during most graphics experiments, we have a better foundation through which to address the fundamental motivation for this research: **using comprehensive empirical testing to validate common design guidelines**. 
Many books and papers provide design guidelines along with examples, redesigns, and sometimes, supporting references to empirical studies  [@JointCommitteeStandards1915;@brewerGuidelinesUsePerceptual1994; @kosslynGraphDesignEye2006;@tufteVisualDisplayQuantitative2001;@kelleherTenGuidelinesEffective2011;@shneidermanEyesHaveIt1996;@craftGuidelinesWhatCan2005;@carrGuidelinesDesigningInformation1999;@munznerVisualizationAnalysisDesign2014;@munzner2009a;@brehmerMultilevelTypologyAbstract2013;@lamEmpiricalStudiesInformation2012;@cardStructureInformationVisualization1997;@steele2010beautiful;@yau2013data;@wong2010wall]; @kandoganGroundedTheoryStudy2016 summarizes the structures and types of guidelines in many of these sources.
There have also been empirical assessments of broad themes common to different sets of guidelines: @ajaniDeclutterFocusEmpirically2022 experimentally evaluated two themes ("declutter" and "focus") using several different assessment methods, finding that focused designs were preferred over decluttered designs, which were preferred over cluttered designs. 
At a fundamental level, however, we have a lot to learn about visualization design: the design guidelines that we are promoting as a discipline are built on fairly limited studies that typically focus on accuracy or response time and do not assess the multiple different levels at which a user might engage with the chart and the underlying data.

Specific methods for testing experimental graphics relevant to this project will be assessed as part of the methods section below, as will relevant preliminary studies that contributed to the development of this project.

<!-- ### Preliminary Studies  -->

<!-- Summarize any relevant preliminary supporting data.  --> 
<!-- Can also be included in research plan --> 

<!-- This is the place to highlight your own preliminary data, showing your ability to develop and test hypotheses, design rigorous experiments, perform experimental techniques, and analyze and interpret data. -->

<!-- • Illustrate the relevance of your preliminary data to your specific aims/goals and how it relates to your proposed research plan. -->

<!-- • Use high quality graphics and tables to illustrate your data and results. -->

## Methods

[Link to Excel Sheet](https://uofnelincoln-my.sharepoint.com/:x:/g/personal/svanderplas2_unl_edu/EWdmaS7it8VMlxCsZ9_XRacB9zIwx2nIQZJoPzoeU3yD6Q?e=H9qwHb&nav=MTVfe0NDMjUzODdELUU4MDAtNDhGOS05OTg2LTZFRDA0NDI0NjFBOH0)

<!-- c. Methods -->
<!-- This is generally the longest section of the research plan, where you develop the details of your project. This section is not just about methods but design. -->
<!-- Suggested format for this section: -->
<!-- D. RESEARCH DESIGN AND METHODS -->
<!-- Introductory paragraph to orient reviewer to what you plan to do. -->

This project is designed to lay a foundation for more robust experimental evaluation of statistical graphics, with the goal of producing more nuanced, user-focused design guidelines that are backed up with empirical evidence.
To address this broad goal, we will benchmark methods for testing statistical graphics that correspond to typical uses for statistical graphics.
In parallel, we will assess the impact of smaller methodological choices on statistical graphics research, which will provide context to conflicting historical experiments. 


Finally, we will leverage 
We will use these methods to assess design guidelines and recommendations, providing a more nuanced, task-focused, empirical evaluation of these design criteria to provide clarity in the importance and impact of design guidelines; if successful this will improve design criteria guidelines, but will also allow us to reconcile the results from conflicting historical experiments.
While these two goals are related, they are not dependent: there are already sufficient methods in the literature for testing graphics to allow us to complete a task-focused evaluation of design guidelines; our contribution in the first aim is to modernize historical methods, fill in gaps in task-focused analysis methods, provide direct comparisons between methods, and create guidelines for the use of each method when testing statistical graphics. 

<!-- When outlining your experimental plan: -->
<!-- • Base it on your specific aims—restate the aims and describe the flow of experiments for each aim. -->
<!-- • Describe methodology for each aim, establishing your expertise with the approach by linking back to your preliminary studies and assessing the strengths and limitations of your approach. -->
<!-- • Cite appropriate references. -->
<!-- • Describe the timeline and sequence of experiments. -->
<!-- • Talk about relevance to the field. -->

<!-- D.1. Aim 1. Repeat Specific Aim from page 1. -->
<!-- D.1.a. Hypothesis and Rationale. -->
<!-- D.1.b. Experimental Plan. -->
<!-- D.1.c. Expected Outcomes/Evaluation -->
<!-- D.1.d. Potential Pitfalls and Alternative Approaches -->
<!-- Discuss the limitations of each approach you are proposing and how they may affect your results and data.  -->
<!-- • Call attention to potential difficulties and propose alternatives you will use if a technique is inadequate or the results are inconclusive. -->
<!-- • Describe potential pitfalls and what you will do if they occur. -->
<!-- • State what you will do if results are negative, how negative findings will also advance the field, and what you will do next. -->
<!-- D.1.e Evaluation - Describe how you will assess your research program. -->
<!-- • Consider linking this to your timeline and/or milestones. -->
<!-- • Describe future plans. -->
<!-- • Identify what others will be able to do with your research results. -->
<!-- D.1.f Future Steps -->

<!-- D.2. Aim 2. Repeat Specific Aim from page 1. -->
<!-- D.2.a. Hypothesis and Rationale. -->
<!-- D.2.b. Experimental Plan. -->
<!-- D.2.c. Expected Outcomes/Evaluation -->
<!-- D.2.d. Potential Pitfalls and Alternative Approaches -->
<!-- Discuss the limitations of each approach you are proposing and how they may affect your results and data.  -->
<!-- • Call attention to potential difficulties and propose alternatives you will use if a technique is inadequate or the results are inconclusive. -->
<!-- • Describe potential pitfalls and what you will do if they occur. -->
<!-- • State what you will do if results are negative, how negative findings will also advance the field, and what you will do next. -->
<!-- D.2.e Evaluation - Describe how you will assess your research program. -->
<!-- • Consider linking this to your timeline and/or milestones. -->
<!-- • Describe future plans. -->
<!-- • Identify what others will be able to do with your research results. -->
<!-- D.2.f Future Steps -->

<!-- D.3. Aim 3. Repeat Specific Aim from page 1. -->
<!-- D.3.a. Hypothesis and Rationale. -->
<!-- D.3.b. Experimental Plan. -->
<!-- D.3.c. Expected Outcomes/Evaluation -->
<!-- D.3.d. Potential Pitfalls and Alternative Approaches -->
<!-- Discuss the limitations of each approach you are proposing and how they may affect your results and data.  -->
<!-- • Call attention to potential difficulties and propose alternatives you will use if a technique is inadequate or the results are inconclusive. -->
<!-- • Describe potential pitfalls and what you will do if they occur. -->
<!-- • State what you will do if results are negative, how negative findings will also advance the field, and what you will do next. -->
<!-- D.3.e Evaluation - Describe how you will assess your research program. -->
<!-- • Consider linking this to your timeline and/or milestones. -->
<!-- • Describe future plans. -->
<!-- • Identify what others will be able to do with your research results. -->
<!-- D.3.f Future Steps -->


# Education Plan
<!-- F. 		Education Plan (Suggested length: 3-4 pages)  -->
<!-- Tips: Use first person, and refer to experiences that have informed your approach to education/teaching/mentoring.  -->

<!-- Describe the educational activities that will be carried out to meet the education objectives set forth in Section A.   -->

<!-- Include clear statements of the education activities to be undertaken.  -->

<!-- Include any plans for collaboration with academia, industry, national laboratories, school and school districts, museums, etc.  -->

<!-- If international/global dimensions are included, clearly state how the education activities will be enhanced by the international engagements and describe the benefits to participants in the U.S. and abroad. Delineate how the activities fit within the context of the expertise, facilities, data, and other resources that are being applied globally in relevant areas of education, and how the CAREER award would position you and UNL to take a leadership role.  -->

<!-- Clearly state the intended impact of all education activities.   -->

<!-- Describe how you plan to evaluate the impact of the educational activities on students and other participants.  -->

<!-- When appropriate, draw on and cite research and best practices in curriculum, pedagogy, and evaluation.  -->

 
## Overview

<!-- a. Overview/background/aims -->
<!-- Frame a big picture for your education plan. Include background, motivation, rationale, and aims for each major activity in the plan (e.g., Aim 1, Aim 2, Aim 3.). Suggest limiting to 3 major aims. -->

## Design and methods

<!-- b. Design and methods -->
<!-- For each aim listed in 3a, describe the target audience, needs/gaps to be addressed, methods, resources (including  personnel, partnerships, budget), and measurable desired outcomes (i.e., what would success look like?). -->

## Evaluation

<!-- c. Evaluation -->
<!-- Describe how you will assess your education program. -->
<!-- • Consider linking this to your timeline. -->
<!-- Tip: Refer to the NSF User Friendly Handbook for Project Evaluation: nsf.gov/pubs/2002/nsf02057/nsf02057.pdf -->

## Integration of Research and Education
<!-- H.  	Integration of Research and Education (Suggested length: 1/4-1/2 page)  -->
<!-- How will your research impact your education goals and how will your education activities feed back into your research? -->
<!-- Briefly summarize how you will integrate research and education. Consider questions such as: How will you recruit and involve graduate and undergraduate students in research activities? What efforts will you make to recruit students from underrepresented minorities? How will you mentor students in your lab? What outreach activities will you organize, and are these activities aimed at or effective in reaching underrepresented demographics?  -->

# Timeline
<!-- G.  	Timeline (Suggested length: 1/4-1/2 page)  -->
<!-- Provide a timeline for the completion of both research and educational activities, along with key milestones.  -->

 
# Broader Impacts
<!-- I.	Broader Impacts (Suggested length: 1/4-1/2 page)  -->
<!-- Discuss the other broader impacts, besides the education activities, that will accrue from the project. Broader impacts may be accomplished through (1) the research itself, (2) activities directly related to the research, or (3) activities supported by, but complementary to, the project.  -->

 
# Results from Prior NSF Support
<!-- J.	Results from Prior NSF Support  -->
<!-- If the you (the PI) have received NSF support with an award end date in the past five years (including any current funding and no cost extensions), information on the award is required, regardless of whether the support was directly related to the proposal or not. In cases where you have received more than one award (excluding amendments to existing awards), you need only report on the one award that is most closely related to the proposal. Support means salary support, as well as any other funding awarded by NSF, including research, Graduate Research Fellowship, Major Research Instrumentation, conference, equipment, travel, and center awards, etc.  -->
<!-- The following information must be provided:  -->
<!-- The NSF award number, amount, and period of support.  -->
<!-- The title of the project.  -->
<!-- A summary of the results of the completed work, including accomplishments, supported by the award. The results must be separately described under two distinct headings: Intellectual Merit and Broader Impacts.  -->
<!-- A listing of the publications resulting from the NSF award (a complete bibliographic citation for each publication must be provided either in this section or in the References Cited section of the proposal); if none, state “No publications were produced under this award.”  -->
<!-- Evidence of research products and their availability, including, but not limited to: data, publications, samples, physical collections, software, and models, as described in any Data Management Plan.  -->
<!-- If the project was recently awarded and therefore no new results exist, describe the major goals and broader impacts of the project. Note that the proposal may contain up to five pages to describe the results.  Results may be summarized in fewer than five pages, which would give the balance of the 15 pages for the Project Description.  -->


::: {.content-visible when-format="pdf"}
\clearpage
:::

# References {.unnumbered}

::: {.content-visible when-format="pdf"}
\newsection{E}
:::
