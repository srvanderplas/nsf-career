---
title: "Project Description"
bibliography: refs.bib
editor: 
  markdown: 
    wrap: sentence
---

::: {.content-visible when-format="pdf"}
\newsection{D}
:::

```{r new, echo=F, fig.width=2, fig.height=1.5,warning=F,message=F,out.width=".25\\textwidth", include = F}
nums<-rnorm(100,0,1)
df<-data.frame(s=1:100,nums)
library(ggplot2)
ggplot(df, aes(x=nums))+
  geom_histogram()+
  theme_classic(base_size=9)
```

<!-- https://vcresearch.berkeley.edu/sites/default/files/inline-files/CAREER_Writing_Guide_2017_final.pdf -->

# Objectives

<!-- 1. Objectives/Specific Aims/Goals -->

<!-- Other names for these are goals, research thrusts, etc. You can call these whatever you like or whatever is most used in your field. -->

<!-- • Try to fit this information on one page. -->

<!-- • Start with a brief problem statement to introduce your research question and state why it is important. -->

<!-- • Put the solution to your research question in context of your overall, long-term career objectives. -->

<!-- • Identify the specific objective for the present proposal. -->

<!-- • Describe your overall hypothesis for your specific objective. -->

<!-- • List your specific aims for how you will accomplish the objectives of your proposal.  -->

<!--    o Each aim is to find out information, not to do a method (a method supports an aim, but is not the purpose of the aim; i.e., the aim should be outcome-oriented). -->

<!--    o Limit yourself to 2-4 aims. -->

<!--    o Be declarative (use short bullet points). -->

<!--    o Make sure aims are not inter-dependent but supportive of each other (i.e., make sure that if the first step of your proposal doesn’t turn out the way you expect, your entire proposal won't fail). -->

<!--    o Link your specific aims to hypotheses, as appropriate. -->

<!-- • Close this section with a statement on your expected outcomes, emphasizing the project's innovation. -->

<!-- • It can be very useful to try to diagram your objective and aims. -->

<!-- Problem statement:  -->

Statistical graphics are powerful and efficient tools to convey data to support human decision making; however, empirical research on graphical perception is sparse relative to the number of decisions necessary to make a good chart; when relevant studies are available, they often use incomparable methods and produce conflicting results.
Many recommendations are based on opinion rather than empirical study, rendering scientific communications sub-optimal or ineffective.
This is alarming, as effective science communication is critical for cultivating public trust in the scientific process and ensuring that decision makers accurately interpret information when making choices which impact people's lives.

Our visual systems are uniquely optimized for efficient assessment of visual information, which is why data visualizations are so effective.
Some disciplines, such as forensic pattern analysis (shoe-prints, firearms, handwriting) take this further, using subjective visual evaluations in place of quantitative methods.
Fundamentally, these comparisons are similar to data visualization: the image encodes numerical data and actionable information is extracted via human perception.
We can use the insights we gain by studying perception of graphics to "close the loop", building empirical, explainable features for pattern evidence that replicate visual comparisons performed by forensic examiners, producing machine-learning algorithms to provide objective, quantitative support for examiner testimony in court.

This CAREER proposal addresses a fundamental research question underpinning both of these problems: *How do humans make decisions supported by data visualizations?* Three research objectives (ROs) will support the overall **research goal** of advancing our understanding of the role of data visualizations in human decision making:

-   **RO1:** Examine effectiveness of charts across different tasks, such as visual search, comparison, prediction, and estimation, by developing multi-modal methods for graphical testing. If successful, this will produce testing methods that focus on specific cognitive tasks related to use of scientific visualizations, and will facilitate comparison of results across experiments and methods.
-   **RO2:** Empirically validate visualization design guidelines, measuring the impact of design decisions on performance of different tasks. If successful, this will produce nuanced guidelines that are task-focused and empirically derived.
-   **RO3:** Engineer statistical features that mimic human perception which will be combined into empirically validated, objective, quantitative tools that support reproducible decision-making in forensic pattern evidence.

Integrated with these research efforts, the overall **education goal** is to cultivate statistical learning and scientific decision making in society.
Three education objectives (EOs) address this goal:

-   **EO1:** Develop and implement experiential learning activities in graphics for undergraduate introductory statistics courses.
-   **EO2:** Create and implement open educational resources (OER) to introduce reproducible science and open-source software development in statistical programming courses.
-   **EO3:** Engage with forensic scientists, lawyers, and judges, evaluating the scientific support for forensic disciplines and promoting open, reproducible scientific practices.

The research, educational activities, and outreach conducted in support of these objectives will advance our understanding of the design, perception, and use of visual data displays.
New suites of task-focused testing methods will facilitate precise, targeted experiments in data visualization, and applying multiple task-focused testing methods to examine design guidelines will facilitate better scientific communication and dissemination of results, building trust in science and leveraging scientific principles.
Measuring the pedagogical effectiveness of experiential learning in undergraduate statistics courses will provide new understanding of effective strategies for STEM outreach and engagement.
Incorporating students into the research process with experiential learning and using graphics to motivate statistical concepts will engage students at a deeper level and open up STEM topics to math-phobic students in a tangible way.
Weaving scientific communication and reproducibility into courses on statistical programming will produce a new generation of scientists equipped with the tools to conduct better, more reproducible research and communicate the results to other scientists and to the public.

# Research Plan

<!-- 2. Research Plan -->

<!-- Note: The research and educational activities do not need to be addressed separately if the relationship between them is such that it works best to present an integrated project where the two are interspersed throughout the Project Description. -->

## Background Information

<!-- a. Background information -->

<!-- This section lets you: -->

<!-- 1. Orient reader to your subject, providing scientific background and context. -->

<!-- 2. Establish the importance and novelty of your project. -->

<!-- 3. Show your knowledge of the area through a solid review and objective citation of prior related work. -->

<!-- 4. Reveal that you are aware of opportunities, gaps, and roadblocks in your field.  -->

<!-- 5. Persuade the reviewers to become invested in your work by showing how your research matters for the field and their own research. -->

<!-- Write this section in nontechnical terms for a broader audience. -->

<!-- Importance of Scientific visualizations -->

Statistical visualizations are a primary way scientists communicate results to other scientists as well as the general public, but we have been arguing about best practices for the visual display of data almost since these graphical forms were created [@wickham2013graphical; @croxtonBarChartsCircle1927; @eellsRelativeMeritsCircles1926].
Different individuals and organizations have released guidelines for graphical design [@JointCommitteeStandards1915; @brewerGuidelinesUsePerceptual1994; @kosslynGraphDesignEye2006; @tufteVisualDisplayQuantitative2001; @kelleherTenGuidelinesEffective2011], but many of these are based on aesthetic preferences, individual experience, or extrapolation from limited empirical studies.
Even when there are empirical studies to support guidelines for choosing one graphical form over another <!-- to increase perceptual accuracy and decrease viewer effort  --> [@cleveland1984; @hofmannGraphicalTestsPower2012; @spenceVisualPsychophysicsSimple1990; see @vanderplasTestingStatisticalCharts2020 for a review], these studies measure different quantities <!-- (estimation accuracy vs. ability to perceive a departure from a null model) --> in different ways <!-- (direct numerical estimation vs. comparative estimation of proportions) -->; the design and implementation differences have a potentially large effect on the generalizability of the results.
Complicating this problem, the design space of visualization user studies is incredibly large [@Abdul-Rahman2020], and studies can address charts from multiple perspectives, including perception, task-based interaction (with different tasks producing different results), structural characteristics, and more [@Bolte2020].
In addition, the test population is an important and under-examined aspect of the graphics experiment design space: we know that expertise as well as disorders such as dyslexia, dyscalculia, and ADHD affect perception, numeracy, and other processes involved in graph comprehension [@cheng_etal18; @chity_etal12; @hokken_etal23]; while these subpopulations can be difficult to test compared to the general population, empirical design recommendations must also address accessibility and audience considerations, as designers already consider these issues [@bako_etal23] but have no firm guidance about best practice.
<!-- Address Perception vs. Task vs. Structure vs. Meta Perception Testing methods (Measures in Visualization Space, Bolte & Bruckner, in Foundations of Data Visualization) --> Relatively few studies directly compare the effect of different approaches for testing graphics, but minor changes to data collection methods can impact accuracy of even simple numerical estimation tasks [@hondaNumberBiasWisdom2022]; it is reasonable to expect that the specifics of the graphical testing procedure used may impact the results, potentially explaining conflicting empirically based recommendations [@croxtonBarChartsCircle1927; @eellsRelativeMeritsCircles1926; @spenceVisualPsychophysicsSimple1990].
Without the ability to integrate results from different types of studies, it is hard to generate scientifically accurate recommendations from existing research.
To bridge this gap, information visualization needs a suite of easily implemented, validated graphical testing methods.
These methods should then be systematically applied to test existing graphical design guidelines, with the intent to produce nuanced recommendations backed by tailored empirical research.

## Preliminary Data

<!-- b. Preliminary data -->

<!-- This is the place to highlight your own preliminary data, showing your ability to develop and test hypotheses, design rigorous experiments, perform experimental techniques, and analyze and interpret data. -->

<!-- • Illustrate the relevance of your preliminary data to your specific aims/goals and how it relates to your proposed research plan. -->

<!-- • Use high quality graphics and tables to illustrate your data and results. -->

During the early stages of the pandemic, log scales were fairly common because they allow comparison of different orders of magnitude on the same chart; however, it was difficult to find studies evaluating how successfully log scales were read and interpreted, though there were studies suggesting that people systematically under-predict exponential growth [@timmers_inverse_1977; @wagenaar_misperception_1975].
In order to address this gap in the literature, I worked with a graduate student, Emily Robinson, to design and execute a series of three studies addressing different tasks related to log scales: could participants detect a difference in rates of exponential growth?
Could they predict future values?
Could they estimate additive and multiplicative differences from an exponential time series?
We examined these questions by testing charts with log and linear scales; in addition to a separate study testing each question above [@robinson_human_2022, peer reviewed articles in progress], we have also published a tool paper [@robinson_etal23] and a method validation paper [@robinson_etal22]. 

```{=html}
![Lineups (log and linear scale) test detection of differences in exponential growth rates.](log-lineups.png)
```

```{=tex}
\begin{wrapfigure}{R}{.5\textwidth}  
\includegraphics{log-lineups.png}
\caption{Lineups (log and linear scale) test detection of differences in exponential growth rates.}
\end{wrapfigure}
```


## Methods

<!-- c. Methods -->

<!-- This is generally the longest section of the research plan, where you develop the details of your project. This section is not just about methods but design. -->

<!-- Suggested format for this section: -->

<!-- D. RESEARCH DESIGN AND METHODS -->

<!-- Introductory paragraph to orient reviewer to what you plan to do. -->

<!-- When outlining your experimental plan: -->

<!-- • Base it on your specific aims—restate the aims and describe the flow of experiments for each aim. -->

<!-- • Describe methodology for each aim, establishing your expertise with the approach by linking back to your preliminary studies and assessing the strengths and limitations of your approach. -->

<!-- • Cite appropriate references. -->

<!-- • Describe the timeline and sequence of experiments. -->

<!-- • Talk about relevance to the field. -->

<!-- D.1. Aim 1. Repeat Specific Aim from page 1. -->

<!-- D.1.a. Hypothesis and Rationale. -->

<!-- D.1.b. Experimental Plan. -->

<!-- D.1.c. Expected Outcomes/Evaluation -->

<!-- D.1.d. Potential Pitfalls and Alternative Approaches -->

<!-- Discuss the limitations of each approach you are proposing and how they may affect your results and data.  -->

<!-- • Call attention to potential difficulties and propose alternatives you will use if a technique is inadequate or the results are inconclusive. -->

<!-- • Describe potential pitfalls and what you will do if they occur. -->

<!-- • State what you will do if results are negative, how negative findings will also advance the field, and what you will do next. -->

<!-- D.1.e Evaluation -->

<!-- Describe how you will assess your research program. -->

<!-- • Consider linking this to your timeline and/or milestones. -->

<!-- • Describe future plans. -->

<!-- • Identify what others will be able to do with your research results. -->

<!-- D.1.f Future Steps -->

# Education Plan

<!-- 3. Education Plan -->

<!-- Suggested length is 3-5 pages. Tips: Use first person, and refer to experiences that have informed your approach to education/teaching/mentoring.  -->

## Overview

<!-- a. Overview/background/aims -->

<!-- Frame a big picture for your education plan. Include background, motivation, rationale, and aims for each major activity in the plan (e.g., Aim 1, Aim 2, Aim 3.). Suggest limiting to 3 major aims. -->

## Design and methods

<!-- b. Design and methods -->

<!-- For each aim listed in 3a, describe the target audience, needs/gaps to be addressed, methods, resources (including  personnel, partnerships, budget), and measurable desired outcomes (i.e., what would success look like?). -->

## Evaluation

<!-- c. Evaluation -->

<!-- Describe how you will assess your education program. -->

<!-- • Consider linking this to your timeline. -->

<!-- Tip: Refer to the NSF User Friendly Handbook for Project Evaluation: nsf.gov/pubs/2002/nsf02057/nsf02057.pdf -->

## Integration of research and Education

<!-- d. Integration of research and education -->

<!-- How will your research impact your education goals and how will your education activities feed back into your research? -->

# Broader Impacts

<!-- 4. Broader Impacts -->

<!-- You must include a separate section that addresses the Broader Impacts of your proposal. -->

<!-- • Must use the section heading "Broader Impacts" somewhere in your proposal. -->

<!-- • Describe the broader impacts that are intrinsic to your research, as well as those that result from your education activities. -->

<!-- • As an alternative to one section on broader impacts, you could have multiple sections throughout the proposal that are paired with each aim. In this case, be sure that you still create a separate section with the heading "Broader Impacts." -->

# Timeline

<!-- 5. Timeline -->

<!-- This is where you show how everything will fit together. -->

<!-- • Propose an appropriate amount of work for the funding period. Reviewers are sensitive to proposals that seem “overly ambitious.” -->

<!-- • Provide a diagram of your timeline over the funding period, showing when you will hit certain milestones, or provide a list of milestones you will achieve in each year. -->

<!-- • Include key milestones for each education aim -->

# Results from Prior NSF Support

<!-- 6. Results from Prior NSF Support -->

<!-- Describe support from past 5 years per GPG instructions. If you have none, write “Not applicable.” Use this format: -->

<!-- (a) The NSF award number, amount and period of support. -->

<!-- (b) The title of the project. -->

<!-- (c) A summary of the results of the completed work, including accomplishments, supported by the award. The results must be separately described under two distinct headings: Intellectual Merit and Broader Impacts. -->

<!-- (d) A listing of the publications resulting from the NSF award. -->

<!-- (e) Evidence of research products and their availability, including, but not limited to: data, publications, samples, physical collections, software, and models, as described in any Data Management Plan. -->

<!-- (f) Write "Not applicable." (This section is for if the proposal is for renewed support.)  -->

<!-- If the project was recently awarded and therefore no new results exist, describe the major goals and broader impacts of the project. -->

::: {.content-visible when-format="pdf"}
\clearpage
:::

# References {.unnumbered}

::: {.content-visible when-format="pdf"}
\newsection{E}
:::
